{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using twitter data\n",
    "\n",
    "Simple Deep Neural Network, that predicts tweet sentiment. The model can be greatly improved using various <br/>\n",
    "word vectorization techniques and Recurrent Neural Networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tflearn as tflearn\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "test_path = \"./data/twitter_test.txt\"\n",
    "training_path = \"./data/twitter_data.txt\"\n",
    "\n",
    "def all_stop_words():\n",
    "    stop_words = stopwords.words('english')\n",
    "    add_stopwords = [\",\", \"*\" , \")\" , \"(\" ,\".\",\"theres\",\"know\",\"one\",\"though\",\"vinci\",\"ive\",\"da\",\"book\",\"im\",\"went\",\n",
    "                    \"potter\",\"brokeback\",\"mountain\",\"harry\",\"code\",\"mission\",\"impossible\",\"movie\",\"movies\",\"i\",\"ya\",\n",
    "                    \"yet\",\"yall\"]\n",
    "    for w in add_stopwords:\n",
    "        stop_words.append(w)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "stop_words = all_stop_words()\n",
    "def training_data_df(path):\n",
    "    \n",
    "    training_data = open(path,mode='r')\n",
    "    training_array =[]\n",
    "    for d in training_data:\n",
    "        training_dict = {}\n",
    "        sent_tweet_array = d.split('\\t')\n",
    "        training_dict['tweet'] = str(sent_tweet_array[1].lower())\n",
    "        training_dict['cleaned_tweet'] =remove_stop_words(sent_tweet_array[1].lower())\n",
    "        training_dict['positive'] =int(sent_tweet_array[0])\n",
    "        training_array.append(training_dict)\n",
    "    training_df = pd.DataFrame(training_array)\n",
    "    return training_df\n",
    "\n",
    "def sentiment(sentiment_prob):\n",
    "    if sentiment_prob[0]>sentiment_prob[1]:\n",
    "        return \"positive\"\n",
    "    return \"negative\"\n",
    "\n",
    "def remove_stop_words(tweet_text):\n",
    "    tweet_text = re.sub(r'[?|$|.|!]',r'',tweet_text)\n",
    "    tweet_text = re.sub(r'[^a-zA-Z0-9 ]',r'',tweet_text)\n",
    "    result = \"\"\n",
    "    for word in tweet_text.split():            \n",
    "        result = result +\" \"+word.lower()\n",
    "\n",
    "    return result.lstrip()\n",
    "\n",
    "def get_word_frequency(tweet_list):\n",
    "    word_dict = {} \n",
    "    result_array=[]\n",
    "    for tw in tweet_list:\n",
    "        for word in tw.split():\n",
    "            if word in word_dict.keys():\n",
    "                word_dict[word] = word_dict[word]+1\n",
    "                continue\n",
    "            word_dict[word] = 1\n",
    "    for k in word_dict.keys():\n",
    "        result_array.append({'word':k, 'count':word_dict[k]})\n",
    "    return pd.DataFrame(result_array,columns=['count','word'])\n",
    "\n",
    "\n",
    "def common_pos_words(df_data):\n",
    "    return common_words_filter(df_data,1)\n",
    "\n",
    "def common_neg_words(df_data):\n",
    "    return common_words_filter(df_data,0)\n",
    "\n",
    "def common_words_filter(df_data,sent):\n",
    "    pos_tweet =df_data[df_data[\"positive\"] ==sent]\n",
    "    all_tweets = pos_tweet[\"tweet\"].str.cat(sep='|')\n",
    "    common_words=[]\n",
    "    for pos_tw in pos_tweet[\"tweet\"]:\n",
    "        for tw in all_tweets.split('|'):\n",
    "            if pos_tw == tw:\n",
    "                continue\n",
    "            tw_list = list(tw.split())\n",
    "            pos_list = list(pos_tw.split())\n",
    "            inter = list(set(tw_list).intersection(pos_list))\n",
    "            for word in inter:\n",
    "                if word not in common_words:\n",
    "                    common_words.append(word)\n",
    "    return common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data: 7087\n"
     ]
    }
   ],
   "source": [
    "training_data = training_data_df(training_path)\n",
    "# test_data = training_data_df(test_path)\n",
    "training_data[\"neg\"] = training_data[\"positive\"].apply(lambda x: 0 if x==1 else 1)\n",
    "vect = CountVectorizer(stop_words=stop_words)\n",
    "print(\"All data:\",training_data.shape[0])\n",
    "# print(\"Test data:\",test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"tweet\"].apply(lambda x:len(x.split())).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 3995\n",
      "negative: 3092\n"
     ]
    }
   ],
   "source": [
    "print(\"positive:\",len(training_data[training_data[\"positive\"]==1]))\n",
    "print(\"negative:\",len(training_data[training_data[\"positive\"]!=1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2233"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = training_data[['positive','neg','cleaned_tweet', 'tweet']]\n",
    "training_data.values\n",
    "len(training_data[\"cleaned_tweet\"].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>neg</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>i feeel like aaa reetard</td>\n",
       "      <td>i feeel like aaa reetard\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>the da vinci code book is just awesome</td>\n",
       "      <td>the da vinci code book is just awesome.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>this was the first clive cussler ive ever read...</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i liked the da vinci code a lot</td>\n",
       "      <td>i liked the da vinci code a lot.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i liked the da vinci code a lot</td>\n",
       "      <td>i liked the da vinci code a lot.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i liked the da vinci code but it ultimatly did...</td>\n",
       "      <td>i liked the da vinci code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>thats not even an exaggeration and at midnight...</td>\n",
       "      <td>that's not even an exaggeration ) and at midni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i loved the da vinci code but now i want somet...</td>\n",
       "      <td>i loved the da vinci code, but now i want some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i thought da vinci code was great same with ki...</td>\n",
       "      <td>i thought da vinci code was great, same with k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>the da vinci code is actually a good movie</td>\n",
       "      <td>the da vinci code is actually a good movie...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i thought the da vinci code was a pretty good ...</td>\n",
       "      <td>i thought the da vinci code was a pretty good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>the da vinci code is one of the most beautiful...</td>\n",
       "      <td>the da vinci code is one of the most beautiful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>the da vinci code is an amazing book do not ge...</td>\n",
       "      <td>the da vinci code is an * amazing * book, do n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>then i turn on the light and the radio and enj...</td>\n",
       "      <td>then i turn on the light and the radio and enj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>the da vinci code was really good</td>\n",
       "      <td>the da vinci code was really good.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i love da vinci code</td>\n",
       "      <td>i love da vinci code....\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i loved da vinci code</td>\n",
       "      <td>i loved da vinci code..\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>to night the da vinci code and a beautiful mind</td>\n",
       "      <td>to night:: the da vinci code and a beautiful m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>the da vinci code is an awesome book</td>\n",
       "      <td>the da vinci code is an awesome book....\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>thing is i enjoyed the da vinci code</td>\n",
       "      <td>thing is, i enjoyed the da vinci code.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    positive  neg                                      cleaned_tweet  \\\n",
       "0          0    1                           i feeel like aaa reetard   \n",
       "1          1    0             the da vinci code book is just awesome   \n",
       "2          1    0  this was the first clive cussler ive ever read...   \n",
       "3          1    0                    i liked the da vinci code a lot   \n",
       "4          1    0                    i liked the da vinci code a lot   \n",
       "5          1    0  i liked the da vinci code but it ultimatly did...   \n",
       "6          1    0  thats not even an exaggeration and at midnight...   \n",
       "7          1    0  i loved the da vinci code but now i want somet...   \n",
       "8          1    0  i thought da vinci code was great same with ki...   \n",
       "9          1    0         the da vinci code is actually a good movie   \n",
       "10         1    0  i thought the da vinci code was a pretty good ...   \n",
       "11         1    0  the da vinci code is one of the most beautiful...   \n",
       "12         1    0  the da vinci code is an amazing book do not ge...   \n",
       "13         1    0  then i turn on the light and the radio and enj...   \n",
       "14         1    0                  the da vinci code was really good   \n",
       "15         1    0                               i love da vinci code   \n",
       "16         1    0                              i loved da vinci code   \n",
       "17         1    0    to night the da vinci code and a beautiful mind   \n",
       "18         1    0               the da vinci code is an awesome book   \n",
       "19         1    0               thing is i enjoyed the da vinci code   \n",
       "\n",
       "                                                tweet  \n",
       "0                          i feeel like aaa reetard\\n  \n",
       "1           the da vinci code book is just awesome.\\n  \n",
       "2   this was the first clive cussler i've ever rea...  \n",
       "3                  i liked the da vinci code a lot.\\n  \n",
       "4                  i liked the da vinci code a lot.\\n  \n",
       "5   i liked the da vinci code but it ultimatly did...  \n",
       "6   that's not even an exaggeration ) and at midni...  \n",
       "7   i loved the da vinci code, but now i want some...  \n",
       "8   i thought da vinci code was great, same with k...  \n",
       "9     the da vinci code is actually a good movie...\\n  \n",
       "10  i thought the da vinci code was a pretty good ...  \n",
       "11  the da vinci code is one of the most beautiful...  \n",
       "12  the da vinci code is an * amazing * book, do n...  \n",
       "13  then i turn on the light and the radio and enj...  \n",
       "14               the da vinci code was really good.\\n  \n",
       "15                         i love da vinci code....\\n  \n",
       "16                          i loved da vinci code..\\n  \n",
       "17  to night:: the da vinci code and a beautiful m...  \n",
       "18         the da vinci code is an awesome book....\\n  \n",
       "19           thing is, i enjoyed the da vinci code.\\n  "
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_processor = tflearn.data_utils.VocabularyProcessor(45)\n",
    "# vocab = vocab_processor.fit(training_data[\"tweet\"])\n",
    "vocab = vocab_processor.fit(training_data[\"cleaned_tweet\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open ('mappings.txt','w') as mappings:\n",
    "    words = []\n",
    "    for sent in training_data[\"cleaned_tweet\"]:\n",
    "        mappings.write(\"{}\\n\".format(sent))\n",
    "#         for word in sent.split():            \n",
    "#             if word in words:\n",
    "#                 continue\n",
    "#             index = vocab.vocabulary_.get(word)\n",
    "#             mappings.write(\"{}\\t{}\\n\".format(word, index))\n",
    "#             words.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2233"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1    2    3 ...,    0    0    0]\n",
      " [   6    7    8 ...,    0    0    0]\n",
      " [  14   15    6 ...,    0    0    0]\n",
      " ..., \n",
      " [ 163    1 2230 ...,    0    0    0]\n",
      " [ 411 1179 1180 ...,    0    0    0]\n",
      " [ 188   26 1179 ...,    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "vocab_array =vocab.transform(training_data[\"cleaned_tweet\"])\n",
    "vocab_array =np.asmatrix(np.array(list(vocab_array)))\n",
    "print(vocab_array)\n",
    "\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = training_data.values[:,:2].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (5315, 45)\n",
      "Test data: (1772, 45)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trX,testX,trY,testY = train_test_split(vocab_array,classes)   \n",
    "# trainY = tflearn.data_utils.to_categorical(trY, nb_classes=2)\n",
    "# testY = tflearn.data_utils.to_categorical(testY, nb_classes=2)\n",
    "\n",
    "print(\"Training data:\",trX.shape)\n",
    "print(\"Test data:\",testX.shape)\n",
    "trY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "\n",
    "\n",
    "Consists of : \n",
    "<ul>\n",
    "<li>Input layer </li> \n",
    "<li>Hidden layer with 300 neurons</li> \n",
    "<li>Output softmax layer, which outputs the likelyhood of a class: [1,0]</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = tflearn.lstm(embedding, 45)\n",
    "net = tflearn.fully_connected(net, 2, activation='sigmoid',regularizer='L2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.17495\u001b[0m\u001b[0m | time: 9.518s\n",
      "| Adam | epoch: 005 | loss: 0.17495 - acc: 0.9549 -- iter: 5312/5315\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.16255\u001b[0m\u001b[0m | time: 9.573s\n",
      "| Adam | epoch: 005 | loss: 0.16255 - acc: 0.9594 -- iter: 5315/5315\n",
      "--\n",
      "INFO:tensorflow:/tmp/tflearn_logs/model.ckpt-835 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/tmp/tflearn_logs/model.ckpt-835 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "num_lstm_units = 100\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "input_data = tflearn.input_data(shape=[None,45],name='input_data')\n",
    "embedding = tflearn.embedding(input_data, input_dim=2235, output_dim=45,name='word_embeddings')\n",
    "net = tflearn.lstm(embedding, num_lstm_units,name='lstm')\n",
    "\n",
    "\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                     loss='categorical_crossentropy',name='regression')\n",
    "model = tflearn.DNN(net,tensorboard_verbose=3,checkpoint_path='/tmp/tflearn_logs/model.ckpt',session=sess)\n",
    "tflearn.helpers.summarize(input_data,type='histogram',name='input_sum')\n",
    "\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "sess.run(init_op)\n",
    "model.fit(trX,trY, show_metric=True,batch_size=32,n_epoch=5)\n",
    "# sess.run(print(embedding))\n",
    "config = projector.ProjectorConfig()\n",
    "\n",
    "# You can add multiple embeddings. Here we add only one.\n",
    "viz_emd = config.embeddings.add()\n",
    "viz_emd.tensor_name = embedding.name\n",
    "# Link this tensor to its metadata file (e.g. labels).\n",
    "viz_emd.metadata_path = os.path.join(\"/tmp/tflearn_logs\", 'metadata.tsv')\n",
    "\n",
    "# Use the same LOG_DIR where you stored your checkpoint.\n",
    "summary_writer = tf.summary.FileWriter(\"/tmp/tflearn_logs\")\n",
    "\n",
    "# The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will\n",
    "# read this file during startup.\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8bf3ecff1273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word_embeddings/embedding_lookup:0'"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import os\n",
    "# Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver([images])\n",
    "\n",
    "#     sess.run(images.initializer)\n",
    "    saver.save(sess, os.path.join(LOG_DIR, 'images.ckpt'))\n",
    "    config = projector.ProjectorConfig()\n",
    "\n",
    "    # You can add multiple embeddings. Here we add only one.\n",
    "    viz_emd = config.embeddings.add()\n",
    "    viz_emd.tensor_name = \"word_embeddings/W\"\n",
    "    # Link this tensor to its metadata file (e.g. labels).\n",
    "    viz_emd.metadata_path = os.path.join(\"/tmp/tflearn_logs\", 'metadata.tsv')\n",
    "\n",
    "    # Use the same LOG_DIR where you stored your checkpoint.\n",
    "    summary_writer = tf.summary.FileWriter(\"/tmp/tflearn_logs\")\n",
    "\n",
    "    # The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will\n",
    "    # read this file during startup.\n",
    "    projector.visualize_embeddings(summary_writer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word_embeddings/W'"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viz_emd.tensor_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "def auc_curve():\n",
    "    test_y=testY[:,0]\n",
    "    pred = []\n",
    "    i=0\n",
    "    for t in testX:\n",
    "        p = model.predict([t])\n",
    "        p_toarray = np.asarray(p)[0]\n",
    "        if p_toarray[0]>p_toarray[1]:\n",
    "            pred.append(1.0)\n",
    "        else:\n",
    "            pred.append(0.0)\n",
    "    print(metrics.classification_report(test_y.astype(float), np.asarray(pred)))\n",
    "    print(metrics.confusion_matrix(test_y.astype(float), pred))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_y, np.asarray(pred))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print('AUC = %0.4f'% roc_auc)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b',\n",
    "    label='AUC = %0.2f'% roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([-0.1,1.2])\n",
    "    plt.ylim([-0.1,1.2])\n",
    "    plt.ylabel('True Positive Sentiment Rate')\n",
    "    plt.xlabel('False Positive Sentiment Rate')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "auc_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: the da vinci code book is just awesome.\n",
      "\n",
      "<generator object VocabularyProcessor.transform at 0x1ab954888>\n",
      "[ 0.9755044  0.1289884]\n",
      "sentiment: positive\n",
      "--------\n",
      "Tweet: this was the first clive cussler i've ever read, but even books like relic, and da vinci code were more plausible than this.\n",
      "\n",
      "<generator object VocabularyProcessor.transform at 0x18227cdb0>\n",
      "[ 0.97549349  0.1290116 ]\n",
      "sentiment: positive\n",
      "--------\n",
      "Tweet: i liked the da vinci code a lot.\n",
      "\n",
      "<generator object VocabularyProcessor.transform at 0x1ab954888>\n",
      "[ 0.97550464  0.1289878 ]\n",
      "sentiment: positive\n",
      "--------\n",
      "Tweet: i liked the da vinci code a lot.\n",
      "\n",
      "<generator object VocabularyProcessor.transform at 0x18227cdb0>\n",
      "[ 0.97550464  0.1289878 ]\n",
      "sentiment: positive\n",
      "--------\n",
      "Tweet: i liked the da vinci code but it ultimatly didn't seem to hold it's own.\n",
      "\n",
      "<generator object VocabularyProcessor.transform at 0x1ab954888>\n",
      "[ 0.97550303  0.1289911 ]\n",
      "sentiment: positive\n",
      "--------\n",
      "Tweet: that's not even an exaggeration ) and at midnight we went to wal-mart to buy the da vinci code, which is amazing of course.\n",
      "\n",
      "<generator object VocabularyProcessor.transform at 0x18227cdb0>\n",
      "[ 0.9754954   0.12900729]\n",
      "sentiment: positive\n",
      "--------\n",
      "Tweet: i loved the da vinci code, but now i want something better and different!..\n",
      "\n",
      "<generator object VocabularyProcessor.transform at 0x1ab954888>\n",
      "[ 0.9755038   0.12898961]\n",
      "sentiment: positive\n",
      "--------\n",
      "Tweet: i thought da vinci code was great, same with kite runner.\n",
      "\n",
      "<generator object VocabularyProcessor.transform at 0x18227cdb0>\n",
      "[ 0.97550392  0.12898925]\n",
      "sentiment: positive\n",
      "--------\n",
      "Tweet: the da vinci code is actually a good movie...\n",
      "\n",
      "<generator object VocabularyProcessor.transform at 0x1ab954888>\n",
      "[ 0.97550344  0.12899031]\n",
      "sentiment: positive\n",
      "--------\n",
      "Tweet: i thought the da vinci code was a pretty good book.\n",
      "\n",
      "<generator object VocabularyProcessor.transform at 0x18227cdb0>\n",
      "[ 0.97550404  0.12898894]\n",
      "sentiment: positive\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in training_data[training_data[\"positive\"] ==1][\"tweet\"].head(10):\n",
    "    print(\"Tweet:\",t)\n",
    "    enc = vocab_processor.fit_transform([t])\n",
    "    print(enc)\n",
    "    p = model.predict(list(enc))\n",
    "    print(np.asarray(p)[0])\n",
    "    print(\"sentiment:\",sentiment(np.asarray(p)[0]))\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: i feeel like aaa reetard\n",
      "\n",
      "[ 0.97550493  0.12898722]\n",
      "sentiment: positive\n",
      "--------\n",
      "Tweet: da vinci code was a terrible movie.\n",
      "\n",
      "[ 0.00477899  0.91768086]\n",
      "sentiment: negative\n",
      "--------\n",
      "Tweet: then again, the da vinci code is super shitty movie, and it made like 700 million.\n",
      "\n",
      "[ 0.0047802   0.91766834]\n",
      "sentiment: negative\n",
      "--------\n",
      "Tweet: the da vinci code comes out tomorrow, which sucks.\n",
      "\n",
      "[ 0.0047802   0.91766834]\n",
      "sentiment: negative\n",
      "--------\n",
      "Tweet: i thought the da vinci code movie was really boring.\n",
      "\n",
      "[ 0.97550493  0.12898722]\n",
      "sentiment: positive\n",
      "--------\n",
      "Tweet: god, yahoo games has this truly-awful looking da vinci code-themed skin on it's chessboard right now.\n",
      "\n",
      "[ 0.97550493  0.12898704]\n",
      "sentiment: positive\n",
      "--------\n",
      "Tweet: da vinci code does suck.\n",
      "\n",
      "[ 0.00477899  0.91768086]\n",
      "sentiment: negative\n",
      "--------\n",
      "Tweet: and better...-we all know da vinci code is bogus and inaccurate.\n",
      "\n",
      "[ 0.97550052  0.12899639]\n",
      "sentiment: positive\n",
      "--------\n",
      "Tweet: last time, da vinci code is also a bit disappointing to me, because many things written in the book is never mentioned in movie.\n",
      "\n",
      "[ 0.97550356  0.12899008]\n",
      "sentiment: positive\n",
      "--------\n",
      "Tweet: and better...-we all know da vinci code is bogus and inaccurate.\n",
      "\n",
      "[ 0.97550052  0.12899639]\n",
      "sentiment: positive\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in training_data[training_data[\"positive\"] ==0][\"tweet\"].head(10):\n",
    "    print(\"Tweet:\",t)\n",
    "    enc = vocab_processor.fit_transform(t)\n",
    "    p = model.predict(list(enc))\n",
    "    print(np.asarray(p)[0])\n",
    "    print(\"sentiment:\",sentiment(np.asarray(p)[0]))\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Besides the tasty food, the service is incredible!\n",
      "[ 0.97550392  0.12898931]\n",
      "sentiment: positive\n",
      "--------\n",
      "Tweet: Horrible service. Absolutely no sense of customer service and how to take an order. Your order will never reach you.\n",
      "[ 0.97550392  0.12898931]\n",
      "sentiment: positive\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "testing_file = open(test_path,mode='r')\n",
    "testing_tweets=[]\n",
    "for x in testing_file:\n",
    "    s= remove_stop_words(x)\n",
    "    testing_tweets.append(s)\n",
    "\n",
    "testing_tweets=[\"Besides the tasty food, the service is incredible!\",\n",
    "                \"Horrible service. Absolutely no sense of customer service and how to take an order. Your order will never reach you.\"]\n",
    "\n",
    "# testing_tweets[100:200]    \n",
    "for t in testing_tweets:\n",
    "    print(\"Tweet:\",t)\n",
    "    enc = vocab_processor.fit_transform(t)\n",
    "    p = model.predict(list(enc))\n",
    "    print(np.asarray(p)[0])\n",
    "    print(\"sentiment:\",sentiment(np.asarray(p)[0]))\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform([\"satisfied\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 64)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = np.random.random([1024, 64])  # 64-dimensional embeddings\n",
    "ids = np.array([0, 5, 17, 33])\n",
    "matrix.shape\n",
    "#print (matrix[ids] ) # prints a matrix of shape [4, 64] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
