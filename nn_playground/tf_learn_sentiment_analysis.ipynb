{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using twitter data\n",
    "\n",
    "Simple Deep Neural Network, that predicts tweet sentiment. The model can be greatly improved using various <br/>\n",
    "word vectorization techniques and Recurrent Neural Networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tflearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import preprocessor as p\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import tflearn as tflearn\n",
    "\n",
    "def all_stop_words():\n",
    "    stop_words = stopwords.words('english')\n",
    "    add_stopwords = [\",\", \"*\" , \")\" , \"(\" ,\".\",\"theres\",\"know\",\"one\",\"though\",\"vinci\",\"ive\",\"da\",\"book\",\"im\",\"went\",\n",
    "                    \"potter\",\"brokeback\",\"mountain\",\"harry\",\"code\",\"mission\",\"impossible\",\"movie\",\"movies\",\"i\",\"ya\",\n",
    "                    \"yet\",\"yall\"]\n",
    "    for w in add_stopwords:\n",
    "        stop_words.append(w)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "stop_words = all_stop_words()\n",
    "def training_data_df():\n",
    "    \n",
    "    training_data = open(\"./data/twitter_data.txt\",mode='r')\n",
    "    training_array =[]\n",
    "    for d in training_data:\n",
    "        training_dict = {}\n",
    "        sent_tweet_array = d.split('\\t')\n",
    "        training_dict['tweet'] = sent_tweet_array[1].lower()\n",
    "        training_dict['cleaned_tweet'] =remove_stop_words(sent_tweet_array[1].lower())\n",
    "        training_dict['sentiment'] =int(sent_tweet_array[0])\n",
    "        training_array.append(training_dict)\n",
    "    training_df = pd.DataFrame(training_array)\n",
    "    return training_df\n",
    "\n",
    "def remove_stop_words(tweet_text):\n",
    "    tweet_text = re.sub(r'[?|$|.|!]',r'',tweet_text)\n",
    "    tweet_text = re.sub(r'[^a-zA-Z0-9 ]',r'',tweet_text)\n",
    "    result = \"\"\n",
    "    for word in tweet_text.split():            \n",
    "        if word not in stop_words:\n",
    "            result = result +\" \"+word.lower()\n",
    "\n",
    "    return result.lstrip()\n",
    "\n",
    "def get_word_frequency(tweet_list):\n",
    "    word_dict = {} \n",
    "    result_array=[]\n",
    "    for tw in tweet_list:\n",
    "        for word in tw.split():\n",
    "            if word in word_dict.keys():\n",
    "                word_dict[word] = word_dict[word]+1\n",
    "                continue\n",
    "            word_dict[word] = 1\n",
    "    for k in word_dict.keys():\n",
    "        result_array.append({'word':k, 'count':word_dict[k]})\n",
    "    return pd.DataFrame(result_array,columns=['count','word'])\n",
    "\n",
    "\n",
    "def common_pos_words(df_data):\n",
    "    return common_words_filter(df_data,1)\n",
    "\n",
    "def common_neg_words(df_data):\n",
    "    return common_words_filter(df_data,0)\n",
    "\n",
    "def common_words_filter(df_data,sent):\n",
    "    pos_tweet =df_data[df_data[\"sentiment\"] ==sent]\n",
    "    all_tweets = pos_tweet[\"tweet\"].str.cat(sep='|')\n",
    "    common_words=[]\n",
    "    for pos_tw in pos_tweet[\"tweet\"]:\n",
    "        for tw in all_tweets.split('|'):\n",
    "            if pos_tw == tw:\n",
    "                continue\n",
    "            tw_list = list(tw.split())\n",
    "            pos_list = list(pos_tw.split())\n",
    "            inter = list(set(tw_list).intersection(pos_list))\n",
    "            for word in inter:\n",
    "                if word not in common_words:\n",
    "                    common_words.append(word)\n",
    "    return common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data: 7086\n"
     ]
    }
   ],
   "source": [
    "training_data = training_data_df()\n",
    "training_data[\"neg\"] = training_data[\"sentiment\"].apply(lambda x: 0 if x==1 else 1)\n",
    "vect = CountVectorizer(stop_words=all_stop_words())\n",
    "print(\"All data:\",training_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awesome</td>\n",
       "      <td>1</td>\n",
       "      <td>the da vinci code book is just awesome.\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>first clive cussler ever read even books like ...</td>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>liked lot</td>\n",
       "      <td>1</td>\n",
       "      <td>i liked the da vinci code a lot.\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>liked lot</td>\n",
       "      <td>1</td>\n",
       "      <td>i liked the da vinci code a lot.\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>liked ultimatly didnt seem hold</td>\n",
       "      <td>1</td>\n",
       "      <td>i liked the da vinci code but it ultimatly did...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>thats even exaggeration midnight walmart buy a...</td>\n",
       "      <td>1</td>\n",
       "      <td>that's not even an exaggeration ) and at midni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>loved want something better different</td>\n",
       "      <td>1</td>\n",
       "      <td>i loved the da vinci code, but now i want some...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>thought great kite runner</td>\n",
       "      <td>1</td>\n",
       "      <td>i thought da vinci code was great, same with k...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>actually good</td>\n",
       "      <td>1</td>\n",
       "      <td>the da vinci code is actually a good movie...\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thought pretty good</td>\n",
       "      <td>1</td>\n",
       "      <td>i thought the da vinci code was a pretty good ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       cleaned_tweet  sentiment  \\\n",
       "0                                            awesome          1   \n",
       "1  first clive cussler ever read even books like ...          1   \n",
       "2                                          liked lot          1   \n",
       "3                                          liked lot          1   \n",
       "4                    liked ultimatly didnt seem hold          1   \n",
       "5  thats even exaggeration midnight walmart buy a...          1   \n",
       "6              loved want something better different          1   \n",
       "7                          thought great kite runner          1   \n",
       "8                                      actually good          1   \n",
       "9                                thought pretty good          1   \n",
       "\n",
       "                                               tweet  neg  \n",
       "0          the da vinci code book is just awesome.\\n    0  \n",
       "1  this was the first clive cussler i've ever rea...    0  \n",
       "2                 i liked the da vinci code a lot.\\n    0  \n",
       "3                 i liked the da vinci code a lot.\\n    0  \n",
       "4  i liked the da vinci code but it ultimatly did...    0  \n",
       "5  that's not even an exaggeration ) and at midni...    0  \n",
       "6  i loved the da vinci code, but now i want some...    0  \n",
       "7  i thought da vinci code was great, same with k...    0  \n",
       "8    the da vinci code is actually a good movie...\\n    0  \n",
       "9  i thought the da vinci code was a pretty good ...    0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 'awesome', 'the da vinci code book is just awesome.\\n'],\n",
       "       [1, 0,\n",
       "        'first clive cussler ever read even books like relic plausible',\n",
       "        \"this was the first clive cussler i've ever read, but even books like relic, and da vinci code were more plausible than this.\\n\"],\n",
       "       [1, 0, 'liked lot', 'i liked the da vinci code a lot.\\n'],\n",
       "       ..., \n",
       "       [0, 1, 'sit watching mtv awards reminded much despised',\n",
       "        'as i sit here, watching the mtv movie awards, i am reminded of how much i despised the movie brokeback mountain.\\n'],\n",
       "       [0, 1, 'ok horrible',\n",
       "        'ok brokeback mountain is such a horrible movie.\\n'],\n",
       "       [0, 1, 'oh terrible',\n",
       "        'oh, and brokeback mountain was a terrible movie.\\n']], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = training_data[['sentiment','neg','cleaned_tweet', 'tweet']]\n",
    "training_data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(7086, 2069)\n"
     ]
    }
   ],
   "source": [
    "vect.fit(training_data[\"cleaned_tweet\"])\n",
    "word_matrix = vect.transform(training_data[\"cleaned_tweet\"]).toarray()\n",
    "print(type(word_matrix))\n",
    "print(word_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features:  2069\n"
     ]
    }
   ],
   "source": [
    "num_features = word_matrix.shape[1]\n",
    "print(\"Total features: \",num_features)\n",
    "classes = training_data.values[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 5314\n"
     ]
    }
   ],
   "source": [
    "trX,testX,trY,testY = train_test_split(word_matrix,classes)   \n",
    "print(\"Training data:\",trX.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "\n",
    "\n",
    "Consists of : \n",
    "<ul>\n",
    "<li>Input layer </li> \n",
    "<li>Hidden layer with 300 neurons</li> \n",
    "<li>Output softmax layer, which outputs the likelyhood of a class: [1,0]</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "input_data = tflearn.input_data(shape=[None, num_features])\n",
    "init_weights = tflearn.initializations.truncated_normal(shape=None,dtype=tf.float32, seed=None)\n",
    "layer1 = tflearn.layers.fully_connected(input_data,300,activation='sigmoid',weights_init=init_weights,regularizer='L2')\n",
    "net  =tflearn.layers.fully_connected(layer1 , 2,activation='softmax',regularizer='L2')\n",
    "net = tflearn.regression(net, optimizer='adam', loss='categorical_crossentropy')\n",
    "model = tflearn.DNN(net,tensorboard_verbose=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.08910\u001b[0m\u001b[0m | time: 5.193s\n",
      "| Adam | epoch: 010 | loss: 0.08910 - acc: 0.9868 -- iter: 5312/5314\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.08663\u001b[0m\u001b[0m | time: 5.255s\n",
      "| Adam | epoch: 010 | loss: 0.08663 - acc: 0.9877 -- iter: 5314/5314\n",
      "--\n",
      "Evaluation:  [0.99153498723329325]\n"
     ]
    }
   ],
   "source": [
    "model.fit(trX,trY,n_epoch=10,show_metric=True)\n",
    "evl = model.evaluate(testX,testY)\n",
    "print(\"Evaluation: \",evl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect.transform([\"awesome\"]).toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: the da vinci code book is just awesome.\n",
      "\n",
      "Probability: [[0.9709868431091309, 0.029013078659772873]]\n",
      "--------\n",
      "Tweet: this was the first clive cussler i've ever read, but even books like relic, and da vinci code were more plausible than this.\n",
      "\n",
      "Probability: [[0.9721359610557556, 0.027863970026373863]]\n",
      "--------\n",
      "Tweet: i liked the da vinci code a lot.\n",
      "\n",
      "Probability: [[0.8217340707778931, 0.1782659888267517]]\n",
      "--------\n",
      "Tweet: i liked the da vinci code a lot.\n",
      "\n",
      "Probability: [[0.8217340707778931, 0.1782659888267517]]\n",
      "--------\n",
      "Tweet: i liked the da vinci code but it ultimatly didn't seem to hold it's own.\n",
      "\n",
      "Probability: [[0.8213279843330383, 0.17867204546928406]]\n",
      "--------\n",
      "Tweet: that's not even an exaggeration ) and at midnight we went to wal-mart to buy the da vinci code, which is amazing of course.\n",
      "\n",
      "Probability: [[0.6641566753387451, 0.33584335446357727]]\n",
      "--------\n",
      "Tweet: i loved the da vinci code, but now i want something better and different!..\n",
      "\n",
      "Probability: [[0.9394335746765137, 0.060566410422325134]]\n",
      "--------\n",
      "Tweet: i thought da vinci code was great, same with kite runner.\n",
      "\n",
      "Probability: [[0.5096086859703064, 0.49039140343666077]]\n",
      "--------\n",
      "Tweet: the da vinci code is actually a good movie...\n",
      "\n",
      "Probability: [[0.6401045322418213, 0.3598955273628235]]\n",
      "--------\n",
      "Tweet: i thought the da vinci code was a pretty good book.\n",
      "\n",
      "Probability: [[0.507274866104126, 0.492725133895874]]\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for t in training_data[training_data[\"sentiment\"] ==1][\"tweet\"].head(10):\n",
    "    print(\"Tweet:\",t)\n",
    "    enc = vect.transform([t]).toarray()\n",
    "    p = model.predict(enc)\n",
    "    print(\"Probability:\",p)\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: da vinci code was a terrible movie.\n",
      "\n",
      "Probability: [[0.19282272458076477, 0.8071773052215576]]\n",
      "---------\n",
      "Tweet: then again, the da vinci code is super shitty movie, and it made like 700 million.\n",
      "\n",
      "Probability: [[0.7964497804641724, 0.20355020463466644]]\n",
      "---------\n",
      "Tweet: the da vinci code comes out tomorrow, which sucks.\n",
      "\n",
      "Probability: [[0.05570179969072342, 0.9442982077598572]]\n",
      "---------\n",
      "Tweet: i thought the da vinci code movie was really boring.\n",
      "\n",
      "Probability: [[0.10949605703353882, 0.890504002571106]]\n",
      "---------\n",
      "Tweet: god, yahoo games has this truly-awful looking da vinci code-themed skin on it's chessboard right now.\n",
      "\n",
      "Probability: [[0.33692947030067444, 0.6630704998970032]]\n",
      "---------\n",
      "Tweet: da vinci code does suck.\n",
      "\n",
      "Probability: [[0.08318359404802322, 0.9168164134025574]]\n",
      "---------\n",
      "Tweet: and better...-we all know da vinci code is bogus and inaccurate.\n",
      "\n",
      "Probability: [[0.41265419125556946, 0.5873458385467529]]\n",
      "---------\n",
      "Tweet: last time, da vinci code is also a bit disappointing to me, because many things written in the book is never mentioned in movie.\n",
      "\n",
      "Probability: [[0.2898087501525879, 0.7101912498474121]]\n",
      "---------\n",
      "Tweet: and better...-we all know da vinci code is bogus and inaccurate.\n",
      "\n",
      "Probability: [[0.41265419125556946, 0.5873458385467529]]\n",
      "---------\n",
      "Tweet: and better..-we all know da vinci code is bogus and inaccurate.\n",
      "\n",
      "Probability: [[0.41265419125556946, 0.5873458385467529]]\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in training_data[training_data[\"sentiment\"] ==0][\"tweet\"].head(10):\n",
    "    print(\"Tweet:\",t)\n",
    "    enc = vect.transform([t]).toarray()\n",
    "    p = model.predict(enc)\n",
    "    print(\"Probability:\",p)\n",
    "    print('---------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: I despise trump\n",
      "Probability: [[0.5055177211761475, 0.4944821894168854]]\n",
      "------------\n",
      "Tweet: The product was bad\n",
      "Probability: [[0.620330810546875, 0.3796692192554474]]\n",
      "------------\n",
      "Tweet: I love this product\n",
      "Probability: [[0.9722011089324951, 0.027798961848020554]]\n",
      "------------\n",
      "Tweet: It was an ok experience\n",
      "Probability: [[0.3718075156211853, 0.6281924247741699]]\n",
      "------------\n",
      "Tweet: Do you hate me ?\n",
      "Probability: [[0.09526906162500381, 0.9047309160232544]]\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "testing_tweets = [\"I despise trump\",\n",
    "                  \"The product was bad\",\n",
    "                  \"I love this product\",\n",
    "                  \"It was an ok experience\",\n",
    "                  \"Do you hate me ?\"]\n",
    "for t in testing_tweets:\n",
    "    print(\"Tweet:\",t)\n",
    "    enc = vect.transform([t]).toarray()\n",
    "    p = model.predict(enc)\n",
    "    print(\"Probability:\",p)\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
